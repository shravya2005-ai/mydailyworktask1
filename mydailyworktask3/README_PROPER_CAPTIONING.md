# Proper Image Captioning with BLIP Model

This is an updated version of the image captioning system that uses the state-of-the-art **BLIP (Bootstrapping Language-Image Pre-training)** model for accurate and natural image captioning.

## ğŸš€ Quick Start

### 1. Setup (First Time Only)
```bash
python setup_proper_captioning.py
```

### 2. Start the Web Server
```bash
python start_captioning_server.py
```

### 3. Open Your Browser
Navigate to: `http://localhost:5000`

## ğŸ¯ What's New

### âœ… Proper Image Captioning
- **BLIP Model**: Uses Salesforce's BLIP model instead of basic classification
- **Natural Language**: Generates human-like, contextual captions
- **High Accuracy**: State-of-the-art vision-language model
- **Multiple Captions**: Generates diverse alternative captions

### âœ… Improved Web Interface
- **Better Results Display**: Shows main caption and alternatives
- **Confidence Scores**: Displays model confidence levels
- **Error Handling**: Graceful error handling and user feedback
- **Responsive Design**: Works on desktop and mobile devices

## ğŸ”§ Technical Details

### Model Architecture
- **BLIP**: Bootstrapping Language-Image Pre-training
- **Vision Encoder**: ViT (Vision Transformer) for image understanding
- **Language Decoder**: BERT-like transformer for caption generation
- **Pre-trained**: Trained on millions of image-text pairs

### Key Features
1. **End-to-End Training**: Unlike the previous classification approach, BLIP is trained specifically for captioning
2. **Contextual Understanding**: Understands relationships between objects in images
3. **Natural Language**: Generates grammatically correct, human-like captions
4. **Multiple Outputs**: Can generate diverse captions for the same image

## ğŸ“ File Structure

```
â”œâ”€â”€ app.py                          # Updated Flask web application
â”œâ”€â”€ proper_captioning.py            # Standalone BLIP captioning class
â”œâ”€â”€ test_blip_model.py              # Test script for BLIP model
â”œâ”€â”€ setup_proper_captioning.py      # Setup and installation script
â”œâ”€â”€ start_captioning_server.py      # Server startup script
â”œâ”€â”€ requirements_web.txt            # Updated web requirements
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index_maroon.html           # Updated web interface
â””â”€â”€ static/
    â””â”€â”€ uploads/                    # Uploaded images storage
```

## ğŸ› ï¸ Installation Details

### Requirements
- Python 3.8+
- PyTorch 2.0+
- Transformers 4.30+
- Flask 2.3+
- PIL/Pillow
- Other dependencies in `requirements_web.txt`

### Manual Installation
```bash
# Install requirements
pip install -r requirements_web.txt

# Test the model
python test_blip_model.py

# Start the server
python app.py
```

## ğŸ§ª Testing

### Test the Model Directly
```bash
python test_blip_model.py
```

### Test with Standalone Script
```bash
python proper_captioning.py
```

### Test the Web Interface
1. Start the server: `python start_captioning_server.py`
2. Open: `http://localhost:5000`
3. Upload an image or use a sample URL

## ğŸ¨ Usage Examples

### Web Interface
1. **Upload Image**: Drag & drop or click to select
2. **Use URL**: Paste image URL and click "Analyze URL"
3. **Try Samples**: Click on sample image buttons

### Programmatic Usage
```python
from app import ProperImageCaptioning

# Initialize captioner
captioner = ProperImageCaptioning()

# Generate caption
image_path = "path/to/image.jpg"
image = captioner.load_image_from_file(image_path)
caption, confidence = captioner.generate_caption(image)

print(f"Caption: {caption}")
print(f"Confidence: {confidence:.2f}")

# Generate multiple captions
multiple_captions = captioner.generate_multiple_captions(image, num_captions=3)
for i, caption in enumerate(multiple_captions):
    print(f"Caption {i+1}: {caption}")
```

## ğŸ” Comparison: Old vs New

| Feature | Old System | New System |
|---------|------------|------------|
| **Model** | ResNet50 Classification | BLIP Vision-Language |
| **Output** | Object labels â†’ Template captions | Natural language captions |
| **Accuracy** | Limited to ImageNet classes | Contextual scene understanding |
| **Captions** | Template-based | Human-like, diverse |
| **Training** | ImageNet classification | Image-text pairs |

### Example Outputs

**Old System (Classification-based):**
- "This image shows a dog, a beautiful dog breed (high confidence: 89.2%)"

**New System (BLIP-based):**
- "a golden retriever sitting in a grassy field with trees in the background"
- "a dog sitting in the grass near some trees"
- "a golden retriever dog sitting on green grass"

## ğŸš¨ Troubleshooting

### Common Issues

1. **Model Download Fails**
   ```bash
   # Check internet connection and try again
   python test_blip_model.py
   ```

2. **CUDA Out of Memory**
   ```python
   # The system automatically uses CPU if GPU memory is insufficient
   # No action needed - it will work on CPU
   ```

3. **Slow First Load**
   ```
   # First model load downloads ~2GB of model files
   # Subsequent loads are much faster
   ```

4. **Import Errors**
   ```bash
   # Reinstall requirements
   pip install -r requirements_web.txt --upgrade
   ```

### Performance Tips

1. **GPU Usage**: Automatically uses GPU if available
2. **Model Caching**: Model is cached after first download
3. **Batch Processing**: Process multiple images efficiently
4. **Memory Management**: Automatic cleanup between requests

## ğŸ”® Future Enhancements

- [ ] Support for video captioning
- [ ] Multi-language caption generation
- [ ] Custom model fine-tuning interface
- [ ] Batch processing for multiple images
- [ ] API endpoints for integration
- [ ] Caption editing and refinement tools

## ğŸ“š References

- [BLIP Paper](https://arxiv.org/abs/2201.12086)
- [Salesforce BLIP Model](https://huggingface.co/Salesforce/blip-image-captioning-base)
- [Hugging Face Transformers](https://huggingface.co/transformers/)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“„ License

This project is open source and available under the MIT License.

---

**Note**: This updated system provides significantly better image captioning results compared to the previous classification-based approach. The BLIP model understands context and generates natural, human-like captions that accurately describe the content and relationships in images.